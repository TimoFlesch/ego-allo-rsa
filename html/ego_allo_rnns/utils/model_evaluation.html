<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.10.0" />
<title>ego_allo_rnns.utils.model_evaluation API documentation</title>
<meta name="description" content="Author: Xuan Wen
Date: 2021-03-16 13:11:17
LastEditTime: 2021-03-30 17:01:04
LastEditors: Please set LastEditors
Description: In User Settings Edit
â€¦" />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js" integrity="sha256-Uv3H6lx7dJmRfRvH8TH6kJD1TSK1aFcwgx+mdg3epi8=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>ego_allo_rnns.utils.model_evaluation</code></h1>
</header>
<section id="section-intro">
<p>Author: Xuan Wen
Date: 2021-03-16 13:11:17
LastEditTime: 2021-03-30 17:01:04
LastEditors: Please set LastEditors
Description: In User Settings Edit
FilePath: /rnn_sc_wc/model_evaluation.py</p>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">&#34;&#34;&#34;
Author: Xuan Wen
Date: 2021-03-16 13:11:17
LastEditTime: 2021-03-30 17:01:04
LastEditors: Please set LastEditors
Description: In User Settings Edit
FilePath: /rnn_sc_wc/model_evaluation.py
&#34;&#34;&#34;

import math

import matplotlib.pyplot as plt
import numpy as np
import seaborn as sns
import torch
import torch.nn as nn
import torchvision.transforms as transforms
from ego_allo_rnns.utils.utils import RSA_predict, front_frame, input_frame, input_label, occlusion



pi = math.pi


def plot_filter(model, img=&#34;input/image.png&#34;):
    model_children = list(model.children())

    # counter to keep count of the conv layers
    counter = 0

    model_weights = []  # we will save the conv layer weights in this list
    conv_layers = []  # we will save the conv layers in this list

    # append all the conv layers and their respective weights to the list
    for i in range(len(model_children)):
        if type(model_children[i]) == nn.Conv2d:
            counter += 1
            model_weights.append(model_children[i].weight)
            conv_layers.append(model_children[i])
        elif type(model_children[i]) == nn.Sequential:
            for child in list(model_children[i]):
                if type(child) == nn.Conv2d:
                    counter += 1
                    model_weights.append(child.weight)
                    conv_layers.append(child)
    print(f&#34;Total convolutional layers: {counter}&#34;)

    # take a look at the conv layers and the respective weights
    for weight, conv in zip(model_weights, conv_layers):
        # print(f&#34;WEIGHT: {weight} \nSHAPE: {weight.shape}&#34;)
        print(f&#34;CONV: {conv} ====&gt; SHAPE: {weight.shape}&#34;)

    # visualize the first conv layer filters
    for layer_index in range(len(model_weights)):
        plt.figure(figsize=(15, 15))
        for i, filter in enumerate(model_weights[layer_index]):
            # (8, 8) because in conv0 we have 7x7 filters and total of 64 (see printed shapes)
            plt.subplot(8, 8, i + 1)
            plt.imshow(filter[0, :, :].detach(), cmap=&#34;gray&#34;)
            plt.axis(&#34;off&#34;)
            plt.title(i)
        plt.suptitle(f&#34;Convolutional Layer Filter No.{layer_index}&#34;, fontsize=32)
        plt.savefig(f&#34;output/filter{layer_index}.png&#34;)

        # define the transforms
    transform = transforms.Compose(
        [
            transforms.ToPILImage(),
            transforms.Resize((100, 100)),
            transforms.ToTensor(),
        ]
    )

    img = np.array(img)
    # apply the transforms
    img = transform(img)
    print(img.size())
    # unsqueeze to add a batch dimension
    img = img.unsqueeze(0)
    print(img.size())

    # pass the image through all the layers
    results = [conv_layers[0](img)]
    for i in range(1, len(conv_layers)):
        # pass the result from the last layer to the next layer
        results.append(conv_layers[i](results[-1]))

    # make a copy of the `results`
    outputs = results

    # visualize 64 features from each layer
    # (although there are more feature maps in the upper layers)
    for num_layer in range(len(outputs)):
        plt.figure(figsize=(30, 30))
        layer_viz = outputs[num_layer][0, :, :, :]
        layer_viz = layer_viz.data
        print(layer_viz.size())
        for i, filter in enumerate(layer_viz):
            if i == 64:  # we will visualize only 8x8 blocks from each layer
                break
            plt.subplot(8, 8, i + 1)
            plt.imshow(filter, cmap=&#34;gray&#34;)
            plt.axis(&#34;off&#34;)
        print(f&#34;Saving layer {num_layer} feature maps...&#34;)
        plt.savefig(f&#34;output/image_layer_{num_layer}.png&#34;)
        # plt.show()
        plt.close()

    pass


def generate_example_image():
    input_type = &#34;SC&#34;
    label_type = &#34;SC&#34;
    frames, start_poke_coordinate, target_poke_coordinate = front_frame(
        random_seed=20, frame_amount=1
    )
    image = input_frame(frames, input_type, start_poke_coordinate)
    label = input_label(
        start_poke_coordinate, target_poke_coordinate, label_type, &#34;Cartesian&#34;
    )

    img = image.astype(np.float32)
    img = np.expand_dims(img, 1)
    img = torch.from_numpy(img)
    plt.imshow(img[0][0])
    plt.show()
    lb = label.astype(np.float32)
    lb = torch.from_numpy(lb)

    plt.imsave(&#34;input/image.png&#34;, img[0][0].tolist(), dpi=100)
    # img = cv.imread(f&#34;../input/{args[&#39;image&#39;]}&#34;)
    # img = cv.cvtColor(img, cv.COLOR_BGR2RGB)
    pass


# def occlusion(
#     model, image=&#34;input/image.png&#34;, occ_size=50, occ_stride=50, occ_pixel=0.2
# ):

#     # get the width and height of the image
#     width, height = 100, 100
#     input_type = &#34;SC&#34;
#     label_type = &#34;SC&#34;
#     frames, start_poke_coordinate, target_poke_coordinate = front_frame(
#         random_seed=20, frame_amount=1
#     )
#     image = input_frame(frames, input_type, start_poke_coordinate)
#     image = np.expand_dims(image, 1)
#     image = torch.from_numpy(image)
#     # setting the output image width and height
#     output_height = int(np.ceil((height - occ_size) / occ_stride))
#     output_width = int(np.ceil((width - occ_size) / occ_stride))

#     # create a white image of sizes we defined
#     heatmap = torch.zeros((output_height, output_width))

#     # iterate all the pixels in each column
#     for h in range(0, height):
#         for w in range(0, width):

#             h_start = h * occ_stride
#             w_start = w * occ_stride
#             h_end = min(height, h_start + occ_size)
#             w_end = min(width, w_start + occ_size)

#             if (w_end) &gt;= width or (h_end) &gt;= height:
#                 continue

#             input_image = image.clone().detach()

#             # replacing all the pixel information in the image with occ_pixel(grey) in the specified location
#             input_image[:, :, w_start:w_end, h_start:h_end] = occ_pixel

#             # run inference on modified image
#             output = model(input_image.float())
#             output = nn.functional.softmax(output, dim=1)
#             prob = np.max(output.tolist())
#             # prob = np.max(output.tolist()[0])

#             # setting the heatmap location to probability value
#             heatmap[h, w] = prob
#
#    return heatmap


def rsa_visualization(rsa, label_type):

    if label_type == &#34;Polar&#34;:
        save_name = &#34;./figures/RSA_SC-SC_SC-WC_polar&#34;
        fig, (ax1, ax2) = plt.subplots(1, 2)
        fig.set_figheight(5)
        fig.set_figwidth(12)
        fig.suptitle(&#34;Comparison between SC-SC model and SC-WC model&#34;, fontsize=16)
        img = ax1.imshow(rsa[0])
        plt.colorbar(img, ax=ax1)
        ax1.set_title(&#34;Predict Theta value&#34;)
        ax1.set(xlabel=&#34;SC-SC&#34;, ylabel=&#34;SC-WC&#34;)
        ax1.set_xticks(range(0, len(rsa[0][0]), math.floor(len(rsa[0][0]) / 4)))
        ax1.set_xticklabels([&#34;0&#34;, &#34;pi/2&#34;, &#34;pi&#34;, &#34;3pi/2&#34;, &#34;2pi&#34;])
        ax1.set_yticks(range(0, len(rsa[0][0]), math.floor(len(rsa[0][0]) / 4)))
        ax1.set_yticklabels([&#34;2pi&#34;, &#34;3pi/2&#34;, &#34;pi&#34;, &#34;pi/2&#34;, &#34;0&#34;])
        img2 = ax2.imshow(rsa[1])
        plt.colorbar(img2, ax=ax2)
        ax2.set_title(&#34;Predict r value&#34;)
        ax2.set(xlabel=&#34;SC-SC&#34;, ylabel=&#34;SC-WC&#34;)
        ax2.set_xticks(range(0, len(rsa[0][0]), math.floor(len(rsa[0][0]) / 4)))
        ax2.set_xticklabels([&#34;0&#34;, &#34;pi/2&#34;, &#34;pi&#34;, &#34;3pi/2&#34;, &#34;2pi&#34;])
        ax2.set_yticks(range(0, len(rsa[0][0]), math.floor(len(rsa[0][0]) / 4)))
        ax2.set_yticklabels([&#34;2pi&#34;, &#34;3pi/2&#34;, &#34;pi&#34;, &#34;pi/2&#34;, &#34;0&#34;])
        plt.show()
    elif label_type == &#34;Cartesian&#34;:
        save_name = &#34;./figures/RSA_SC-SC_SC-WC_Carte&#34;
        fig, (ax1, ax2) = plt.subplots(1, 2)
        fig.set_figheight(5)
        fig.set_figwidth(12)
        fig.suptitle(&#34;Comparison between SC-SC model and SC-WC model&#34;, fontsize=16)
        img = ax1.imshow(rsa[0])
        plt.colorbar(img, ax=ax1)
        ax1.set_title(&#34;Predict X value&#34;)
        ax1.set(xlabel=&#34;SC-SC&#34;, ylabel=&#34;SC-WC&#34;)
        ax1.set_xticks(range(0, len(rsa[0][0]), math.floor(len(rsa[0][0]) / 4)))
        ax1.set_xticklabels([&#34;0&#34;, &#34;0.25&#34;, &#34;0.5&#34;, &#34;0.75&#34;, &#34;1&#34;])
        ax1.set_yticks(range(0, len(rsa[0][0]), math.floor(len(rsa[0][0]) / 4)))
        ax1.set_yticklabels([&#34;1&#34;, &#34;0.75&#34;, &#34;0.5&#34;, &#34;0.25&#34;, &#34;0&#34;])
        img2 = ax2.imshow(rsa[1])
        plt.colorbar(img2, ax=ax2)
        ax2.set_title(&#34;Predict Y value&#34;)
        ax2.set(xlabel=&#34;SC-SC&#34;, ylabel=&#34;SC-WC&#34;)
        ax2.set_xticks(range(0, len(rsa[0][0]), math.floor(len(rsa[0][0]) / 4)))
        ax2.set_xticklabels([&#34;0&#34;, &#34;0.25&#34;, &#34;0.5&#34;, &#34;0.75&#34;, &#34;1&#34;])
        ax2.set_yticks(range(0, len(rsa[0][0]), math.floor(len(rsa[0][0]) / 4)))
        ax2.set_yticklabels([&#34;1&#34;, &#34;0.75&#34;, &#34;0.5&#34;, &#34;0.25&#34;, &#34;0&#34;])
        plt.savefig(save_name + &#34;.png&#34;, dpi=400)
        plt.show()

    pass


if __name__ == &#34;__main__&#34;:
    device = torch.device(&#34;cuda:0&#34; if torch.cuda.is_available() else &#34;cpu&#34;)
    generate_example_image()
    # ---------------------------------------------------------------------------- #
    #                                  load model                                  #
    # ---------------------------------------------------------------------------- #

    model = ConvNet()
    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)
    checkpoint = torch.load(&#34;model.pt&#34;)
    model.load_state_dict(checkpoint[&#34;model_state_dict&#34;])
    optimizer.load_state_dict(checkpoint[&#34;optimizer_state_dict&#34;])
    epoch = checkpoint[&#34;epoch&#34;]

    model2 = ConvNet()
    optimizer2 = torch.optim.Adam(model.parameters(), lr=0.001)
    checkpoint2 = torch.load(&#34;model2.pt&#34;)
    model2.load_state_dict(checkpoint2[&#34;model_state_dict&#34;])
    optimizer2.load_state_dict(checkpoint2[&#34;optimizer_state_dict&#34;])
    epoch2 = checkpoint2[&#34;epoch&#34;]

    # ---------------------------------------------------------------------------- #
    #                               occlusion heatmap                              #
    # ---------------------------------------------------------------------------- #
    model = model.float()
    heatmap = occlusion(model, occ_size=10, occ_stride=5)
    print(heatmap.shape)
    print(heatmap)
    imgplot = sns.heatmap(heatmap, xticklabels=False, yticklabels=False)
    figure = imgplot.get_figure()
    figure.savefig(&#34;./figures/svm_conf.png&#34;, dpi=400)

    # ---------------------------------------------------------------------------- #
    #                                 RSA analysis                                 #
    # ---------------------------------------------------------------------------- #
    rsa = RSA_predict(model, model2)
    rsa_visualization(rsa, &#34;Cartesian&#34;)</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-functions">Functions</h2>
<dl>
<dt id="ego_allo_rnns.utils.model_evaluation.generate_example_image"><code class="name flex">
<span>def <span class="ident">generate_example_image</span></span>(<span>)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def generate_example_image():
    input_type = &#34;SC&#34;
    label_type = &#34;SC&#34;
    frames, start_poke_coordinate, target_poke_coordinate = front_frame(
        random_seed=20, frame_amount=1
    )
    image = input_frame(frames, input_type, start_poke_coordinate)
    label = input_label(
        start_poke_coordinate, target_poke_coordinate, label_type, &#34;Cartesian&#34;
    )

    img = image.astype(np.float32)
    img = np.expand_dims(img, 1)
    img = torch.from_numpy(img)
    plt.imshow(img[0][0])
    plt.show()
    lb = label.astype(np.float32)
    lb = torch.from_numpy(lb)

    plt.imsave(&#34;input/image.png&#34;, img[0][0].tolist(), dpi=100)
    # img = cv.imread(f&#34;../input/{args[&#39;image&#39;]}&#34;)
    # img = cv.cvtColor(img, cv.COLOR_BGR2RGB)
    pass</code></pre>
</details>
</dd>
<dt id="ego_allo_rnns.utils.model_evaluation.plot_filter"><code class="name flex">
<span>def <span class="ident">plot_filter</span></span>(<span>model, img='input/image.png')</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def plot_filter(model, img=&#34;input/image.png&#34;):
    model_children = list(model.children())

    # counter to keep count of the conv layers
    counter = 0

    model_weights = []  # we will save the conv layer weights in this list
    conv_layers = []  # we will save the conv layers in this list

    # append all the conv layers and their respective weights to the list
    for i in range(len(model_children)):
        if type(model_children[i]) == nn.Conv2d:
            counter += 1
            model_weights.append(model_children[i].weight)
            conv_layers.append(model_children[i])
        elif type(model_children[i]) == nn.Sequential:
            for child in list(model_children[i]):
                if type(child) == nn.Conv2d:
                    counter += 1
                    model_weights.append(child.weight)
                    conv_layers.append(child)
    print(f&#34;Total convolutional layers: {counter}&#34;)

    # take a look at the conv layers and the respective weights
    for weight, conv in zip(model_weights, conv_layers):
        # print(f&#34;WEIGHT: {weight} \nSHAPE: {weight.shape}&#34;)
        print(f&#34;CONV: {conv} ====&gt; SHAPE: {weight.shape}&#34;)

    # visualize the first conv layer filters
    for layer_index in range(len(model_weights)):
        plt.figure(figsize=(15, 15))
        for i, filter in enumerate(model_weights[layer_index]):
            # (8, 8) because in conv0 we have 7x7 filters and total of 64 (see printed shapes)
            plt.subplot(8, 8, i + 1)
            plt.imshow(filter[0, :, :].detach(), cmap=&#34;gray&#34;)
            plt.axis(&#34;off&#34;)
            plt.title(i)
        plt.suptitle(f&#34;Convolutional Layer Filter No.{layer_index}&#34;, fontsize=32)
        plt.savefig(f&#34;output/filter{layer_index}.png&#34;)

        # define the transforms
    transform = transforms.Compose(
        [
            transforms.ToPILImage(),
            transforms.Resize((100, 100)),
            transforms.ToTensor(),
        ]
    )

    img = np.array(img)
    # apply the transforms
    img = transform(img)
    print(img.size())
    # unsqueeze to add a batch dimension
    img = img.unsqueeze(0)
    print(img.size())

    # pass the image through all the layers
    results = [conv_layers[0](img)]
    for i in range(1, len(conv_layers)):
        # pass the result from the last layer to the next layer
        results.append(conv_layers[i](results[-1]))

    # make a copy of the `results`
    outputs = results

    # visualize 64 features from each layer
    # (although there are more feature maps in the upper layers)
    for num_layer in range(len(outputs)):
        plt.figure(figsize=(30, 30))
        layer_viz = outputs[num_layer][0, :, :, :]
        layer_viz = layer_viz.data
        print(layer_viz.size())
        for i, filter in enumerate(layer_viz):
            if i == 64:  # we will visualize only 8x8 blocks from each layer
                break
            plt.subplot(8, 8, i + 1)
            plt.imshow(filter, cmap=&#34;gray&#34;)
            plt.axis(&#34;off&#34;)
        print(f&#34;Saving layer {num_layer} feature maps...&#34;)
        plt.savefig(f&#34;output/image_layer_{num_layer}.png&#34;)
        # plt.show()
        plt.close()

    pass</code></pre>
</details>
</dd>
<dt id="ego_allo_rnns.utils.model_evaluation.rsa_visualization"><code class="name flex">
<span>def <span class="ident">rsa_visualization</span></span>(<span>rsa, label_type)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def rsa_visualization(rsa, label_type):

    if label_type == &#34;Polar&#34;:
        save_name = &#34;./figures/RSA_SC-SC_SC-WC_polar&#34;
        fig, (ax1, ax2) = plt.subplots(1, 2)
        fig.set_figheight(5)
        fig.set_figwidth(12)
        fig.suptitle(&#34;Comparison between SC-SC model and SC-WC model&#34;, fontsize=16)
        img = ax1.imshow(rsa[0])
        plt.colorbar(img, ax=ax1)
        ax1.set_title(&#34;Predict Theta value&#34;)
        ax1.set(xlabel=&#34;SC-SC&#34;, ylabel=&#34;SC-WC&#34;)
        ax1.set_xticks(range(0, len(rsa[0][0]), math.floor(len(rsa[0][0]) / 4)))
        ax1.set_xticklabels([&#34;0&#34;, &#34;pi/2&#34;, &#34;pi&#34;, &#34;3pi/2&#34;, &#34;2pi&#34;])
        ax1.set_yticks(range(0, len(rsa[0][0]), math.floor(len(rsa[0][0]) / 4)))
        ax1.set_yticklabels([&#34;2pi&#34;, &#34;3pi/2&#34;, &#34;pi&#34;, &#34;pi/2&#34;, &#34;0&#34;])
        img2 = ax2.imshow(rsa[1])
        plt.colorbar(img2, ax=ax2)
        ax2.set_title(&#34;Predict r value&#34;)
        ax2.set(xlabel=&#34;SC-SC&#34;, ylabel=&#34;SC-WC&#34;)
        ax2.set_xticks(range(0, len(rsa[0][0]), math.floor(len(rsa[0][0]) / 4)))
        ax2.set_xticklabels([&#34;0&#34;, &#34;pi/2&#34;, &#34;pi&#34;, &#34;3pi/2&#34;, &#34;2pi&#34;])
        ax2.set_yticks(range(0, len(rsa[0][0]), math.floor(len(rsa[0][0]) / 4)))
        ax2.set_yticklabels([&#34;2pi&#34;, &#34;3pi/2&#34;, &#34;pi&#34;, &#34;pi/2&#34;, &#34;0&#34;])
        plt.show()
    elif label_type == &#34;Cartesian&#34;:
        save_name = &#34;./figures/RSA_SC-SC_SC-WC_Carte&#34;
        fig, (ax1, ax2) = plt.subplots(1, 2)
        fig.set_figheight(5)
        fig.set_figwidth(12)
        fig.suptitle(&#34;Comparison between SC-SC model and SC-WC model&#34;, fontsize=16)
        img = ax1.imshow(rsa[0])
        plt.colorbar(img, ax=ax1)
        ax1.set_title(&#34;Predict X value&#34;)
        ax1.set(xlabel=&#34;SC-SC&#34;, ylabel=&#34;SC-WC&#34;)
        ax1.set_xticks(range(0, len(rsa[0][0]), math.floor(len(rsa[0][0]) / 4)))
        ax1.set_xticklabels([&#34;0&#34;, &#34;0.25&#34;, &#34;0.5&#34;, &#34;0.75&#34;, &#34;1&#34;])
        ax1.set_yticks(range(0, len(rsa[0][0]), math.floor(len(rsa[0][0]) / 4)))
        ax1.set_yticklabels([&#34;1&#34;, &#34;0.75&#34;, &#34;0.5&#34;, &#34;0.25&#34;, &#34;0&#34;])
        img2 = ax2.imshow(rsa[1])
        plt.colorbar(img2, ax=ax2)
        ax2.set_title(&#34;Predict Y value&#34;)
        ax2.set(xlabel=&#34;SC-SC&#34;, ylabel=&#34;SC-WC&#34;)
        ax2.set_xticks(range(0, len(rsa[0][0]), math.floor(len(rsa[0][0]) / 4)))
        ax2.set_xticklabels([&#34;0&#34;, &#34;0.25&#34;, &#34;0.5&#34;, &#34;0.75&#34;, &#34;1&#34;])
        ax2.set_yticks(range(0, len(rsa[0][0]), math.floor(len(rsa[0][0]) / 4)))
        ax2.set_yticklabels([&#34;1&#34;, &#34;0.75&#34;, &#34;0.5&#34;, &#34;0.25&#34;, &#34;0&#34;])
        plt.savefig(save_name + &#34;.png&#34;, dpi=400)
        plt.show()

    pass</code></pre>
</details>
</dd>
</dl>
</section>
<section>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="ego_allo_rnns.utils" href="index.html">ego_allo_rnns.utils</a></code></li>
</ul>
</li>
<li><h3><a href="#header-functions">Functions</a></h3>
<ul class="">
<li><code><a title="ego_allo_rnns.utils.model_evaluation.generate_example_image" href="#ego_allo_rnns.utils.model_evaluation.generate_example_image">generate_example_image</a></code></li>
<li><code><a title="ego_allo_rnns.utils.model_evaluation.plot_filter" href="#ego_allo_rnns.utils.model_evaluation.plot_filter">plot_filter</a></code></li>
<li><code><a title="ego_allo_rnns.utils.model_evaluation.rsa_visualization" href="#ego_allo_rnns.utils.model_evaluation.rsa_visualization">rsa_visualization</a></code></li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc" title="pdoc: Python API documentation generator"><cite>pdoc</cite> 0.10.0</a>.</p>
</footer>
</body>
</html>